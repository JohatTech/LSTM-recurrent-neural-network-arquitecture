{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohatTech/LSTM-recurrent-neural-network-arquitecture/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K8mwHP9IBSJ"
      },
      "source": [
        "#LSTM neural network arquitecture\n",
        "this is the my first project of recurrent neural network.\n",
        "\n",
        "In this project I will build a LSTM arquitecture to identify basic emotions in amazon products reviews both positive(happy, excited, etc.) and negative (sad, worried, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3cpkZvKhzH"
      },
      "source": [
        "Importing the basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W-NDL9pBKoi4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re\n",
        "import keras\n",
        "from keras import Model\n",
        "from keras.layers import Flatten, LSTM,  Dense, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras.initializers import glorot_uniform\n",
        "from sklearn import model_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsGWLp3uLgMe"
      },
      "source": [
        "# **Loading the data**\n",
        "The dataset I will use is the amazon product review that is stored on the tensorflow database.\n",
        "\n",
        "Then I will convert to a pandas dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShvhoKqQL5CE",
        "outputId": "5701a23e-023f-4296-f090-e5f6a227ece7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_OptionsDataset element_spec={'data': {'customer_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'helpful_votes': TensorSpec(shape=(), dtype=tf.int32, name=None), 'marketplace': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_category': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_parent': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'review_body': TensorSpec(shape=(), dtype=tf.string, name=None), 'review_date': TensorSpec(shape=(), dtype=tf.string, name=None), 'review_headline': TensorSpec(shape=(), dtype=tf.string, name=None), 'review_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'star_rating': TensorSpec(shape=(), dtype=tf.int32, name=None), 'total_votes': TensorSpec(shape=(), dtype=tf.int32, name=None), 'verified_purchase': TensorSpec(shape=(), dtype=tf.int64, name=None), 'vine': TensorSpec(shape=(), dtype=tf.int64, name=None)}}>\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tdfs\n",
        "data = tdfs.load('amazon_us_reviews/Mobile_Electronics_v1_00', split='train', shuffle_files=True)\n",
        "assert isinstance(data, tf.data.Dataset)#check is the dataset still exist in the dataset \n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GchP8rJvVUln"
      },
      "outputs": [],
      "source": [
        "df = tdfs.as_dataframe(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "EVXHkLZrYQ7P"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] =  df['data/star_rating'].apply(lambda score: \"positive\" if score >=3 else \"negative\")\n",
        "\n",
        "df['sentiment'] = df['sentiment'].map({'positive':1, \"negative\":0})\n",
        "\n",
        "df['short_review'] =df['data/review_body'].str.decode(\"utf-8\")\n",
        "df = df[[\"short_review\", \"sentiment\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping last n rows using drop\n",
        "n = 54975\n",
        "df.drop(df.tail(n).index,\n",
        "        inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap95YJ8-rsHB",
        "outputId": "0801ec08-7bed-46c5-ef4c-9a60d029bdce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization of the text and convertion into tokens**\n",
        "Since the data is about text, machine learning models understand numberical features, I need to convert the review columns into numerical values. \n",
        "\n",
        "In this application,I'm going to use the Tokenizer class from Keras preprocessing\n",
        "\n",
        "The tokenizer will convert each word in a sentence into integer tokens/IDs based on the frequency of the word appearing in the corpus.\n",
        "\n",
        "I will convert our feature column and label into a set of lists as thatâ€™s how our Tokenizer wants our data. To apply the Tokenizer to the corpus, I will split the dataset into training and testing sets as the tokenizer only needs to be fit on the training set and not on the test set."
      ],
      "metadata": {
        "id": "BwPoRkOcsd-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into 70% train and 30% test\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(df['short_review'],df['sentiment'], test_size=0.30)"
      ],
      "metadata": {
        "id": "mZfePcqvrsne"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to array \n",
        "\n",
        "x_train = np.array(x_train.values.tolist())\n",
        "x_test = np.array(x_test.values.tolist())\n",
        "y_train = np.array(y_train.values.tolist())\n",
        "y_test = np.array(y_test.values.tolist())"
      ],
      "metadata": {
        "id": "y7jtCiSq0hbH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "word_index = tokenizer.word_index\n",
        "total_size =  len(word_index)+1\n",
        "print(total_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsOdldpy1O_J",
        "outputId": "c436d30f-2ea8-4640-9895-3458fc4bcf96"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text to sequences\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "#add padding to ensure the same length\n",
        "max_length = 100\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=max_length)"
      ],
      "metadata": {
        "id": "0Mv5ieu31v5C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I going to start building the model, since the goal is to build a LSTM arquitecture, the structure of the model is as follow:\n",
        "\n",
        "\n",
        "\n",
        "1.  The Keras embedding layer will be trained with the word that we get from the reviews, initializing with random weights and is defined as the first hidden layers of the network\n",
        "2.   EThe LSTM layer are going to learn what emotion are represented int the words if is negative or positive.\n",
        "3.   The last one is the Dense layer which is the fully connected layer that ensure that the model learn well and improve his accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "p8dm0l6RwJ1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_size, 20, input_length=100))\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation ='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNBmPlrUxaAx",
        "outputId": "35c0fd57-e0d2-4fe4-95b9-8f51f80f8faa"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 20)           676120    \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 32)                6784      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 682,937\n",
            "Trainable params: 682,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=128, epochs=5, verbose=1, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLSD8qzi2TZK",
        "outputId": "3a1b8212-aba5-4a64-e638-f0b46a6d9dbd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "274/274 [==============================] - 318s 1s/step - loss: 0.5591 - acc: 0.7597 - val_loss: 0.5437 - val_acc: 0.7619\n",
            "Epoch 2/5\n",
            "274/274 [==============================] - 296s 1s/step - loss: 0.5139 - acc: 0.7791 - val_loss: 0.4119 - val_acc: 0.8467\n",
            "Epoch 3/5\n",
            "274/274 [==============================] - 291s 1s/step - loss: 0.4502 - acc: 0.8195 - val_loss: 0.4589 - val_acc: 0.8143\n",
            "Epoch 4/5\n",
            "274/274 [==============================] - 292s 1s/step - loss: 0.4570 - acc: 0.8059 - val_loss: 0.4550 - val_acc: 0.8062\n",
            "Epoch 5/5\n",
            "274/274 [==============================] - 335s 1s/step - loss: 0.4357 - acc: 0.8221 - val_loss: 0.4583 - val_acc: 0.8338\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff55a82c450>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance analysis**\n",
        "\n",
        "after trainning the model with 5 epochs and 128 batches I got a loss of 43% and a accuracy of 82.1% on the training side and a loss of 45%  and accuracy of 83.3% in the validation data, which is a great indicator of good performance\n"
      ],
      "metadata": {
        "id": "J1PAgF4J9tyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "That's it for the project, in this case I better understand now the arquitecture of LSTM models and the advantage tha this have over sequencial data, this model need more hiper-parameter tunning and testing with real-life examples from customer reviews, also we can also compare the this model, with other NLP tools like transformer for example, but this will be in a future project. you can download the H5 file in the GitHub repository to use the pre-trained model to futher analysis."
      ],
      "metadata": {
        "id": "87hf_HBc-hsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "wzJpEDCKxaKL"
      },
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": [],
      "mount_file_id": "19yDj-edb79M4npKZcYhY2w8AQdk4Mvvj",
      "authorship_tag": "ABX9TyOR6zpiAovoHWASFXwGBFcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}